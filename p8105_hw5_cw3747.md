p8105_hw5_cw3747
================
chuhan wang_cw3747
2025-11-04

``` r
library(tidyverse)
```

    ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
    ## ✔ dplyr     1.1.4     ✔ readr     2.1.5
    ## ✔ forcats   1.0.0     ✔ stringr   1.5.1
    ## ✔ ggplot2   3.5.2     ✔ tibble    3.3.0
    ## ✔ lubridate 1.9.4     ✔ tidyr     1.3.1
    ## ✔ purrr     1.1.0     
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()
    ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

``` r
library(patchwork)
library(scales)
```

    ## 
    ## Attaching package: 'scales'
    ## 
    ## The following object is masked from 'package:purrr':
    ## 
    ##     discard
    ## 
    ## The following object is masked from 'package:readr':
    ## 
    ##     col_factor

``` r
library(dplyr)


knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem1

### Birthday Function

``` r
library(ggplot2)
library(dplyr)

set.seed(123)


birthday_dup <- function(n, days = 365) {
  bdays <- sample.int(days, n, replace = TRUE) 
  any(duplicated(bdays))                       
}
```

### iteration

``` r
simulate_prob <- function(n, B = 10000, days = 365) {
  mean(replicate(B, birthday_dup(n, days)))
}

ns <- 2:50
B  <- 10000

probs <- numeric(length(ns))
for (i in seq_along(ns)) {
  probs[i] <- simulate_prob(ns[i], B = B)
}

df_loop <- data.frame(n = ns, p = probs)

n50_loop <- min(df_loop$n[df_loop$p >= 0.5])
```

### ggplot

``` r
ggplot(df_loop, aes(n, p)) +
  geom_line(linewidth = 1) +
  geom_point(size = 1.5) +
  geom_hline(yintercept = 0.5, linetype = 2) +
  geom_vline(xintercept = n50_loop, linetype = 2) +
  annotate("text", x = n50_loop + 1, y = 0.52,
           label = paste0("~50% at n = ", n50_loop), hjust = 0) +
  labs(title = "P(at least one shared birthday) vs. Group size",
       x = "Group size (n)", y = "Probability") +
  theme_minimal(base_size = 12)
```

<img src="p8105_hw5_cw3747_files/figure-gfm/unnamed-chunk-4-1.png" width="90%" />

The plot shows that the probability of at least one shared birthday
increases rapidly with group size. Around n = 23, the probability
reaches about 0.5, meaning that in a random group of 23 people there is
50% chance that some pair shares a birthday (this does not mean that
“half of the people” have a match). As n approaches 50, the probability
nears 1, implying that a shared-birthday pair is almost guaranteed,
though not that everyone finds a match. Real-world seasonality and
clustering can shift the curve slightly, but the qualitative “birthday
paradox” pattern remains.

## Problem2

### set function

``` r
library(broom) 

set.seed(123)

sim_once <- function(mu, n = 30, sigma = 5) {
  x  <- rnorm(n, mean = mu, sd = sigma)
  tt <- t.test(x, mu = 0)
  tibble(
    mu = mu,
    mu_hat = mean(x),
    p = tidy(tt)$p.value,
    reject = p < 0.05
  )
}
```

### iteration

``` r
simulate_mu <- function(mu, B = 5000, n = 30, sigma = 5) {
  bind_rows(replicate(B, sim_once(mu, n, sigma), simplify = FALSE))
}
```

### for different $\mu$

``` r
mus <- 0:6
res_list <- vector("list", length(mus))
for (i in seq_along(mus)) {
  res_list[[i]] <- simulate_mu(mus[i], B = 5000)
}
res <- bind_rows(res_list)
```

### power of test

``` r
summ <- res |>
  group_by(mu) |>
  summarise(
    power = mean(reject),
    avg_mu_hat_all = mean(mu_hat),
    avg_mu_hat_rej = mean(mu_hat[reject]),
    .groups = "drop"
  )

ggplot(summ, aes(mu, power)) +
  geom_line(linewidth = 1) +
  geom_point(size = 1.6) +
  geom_hline(yintercept = 0.05, linetype = 2) +
  labs(title = "Power of t-test",
       x = "True mean (μ)", y = "Power") +
  theme_minimal(base_size = 12)
```

<img src="p8105_hw5_cw3747_files/figure-gfm/unnamed-chunk-8-1.png" width="90%" />

We fix $\mu_0 = 0$ and $\sigma = 5$. The effect size is
$d = (\mu - \mu_0) / \sigma$. Hence, larger $\mu$ implies a larger
effect size. The plot shows that power increases monotonically with
$\mu$: at $\mu$ = 0, the the rejection rate is about the rejection rate
is about $\alpha$ = 0.05; as $\mu$ grows, power rises quickly. Larger
effect size makes the sample mean more distinct from 0, yielding a
larger absolute t-statistic and a higher chance of rejecting the null.

### average estimate

``` r
summ_long <- summ |>
  pivot_longer(-mu, names_to = "metric", values_to = "value") |>
  filter(metric %in% c("avg_mu_hat_all","avg_mu_hat_rej"))

ggplot(summ_long, aes(mu, value, color = metric)) +
  geom_line(linewidth = 1) +
  geom_point(size = 1.6) +
  geom_abline(slope = 1, intercept = 0, linetype = 3) +
  scale_color_discrete(labels = c("Overall E[μ̂]", "E[μ̂ | reject]")) +
  labs(title = "Average estimate of μ̂ vs true μ",
       x = "True mean (μ)", y = "Average μ̂", color = "") +
  theme_minimal(base_size = 12)
```

<img src="p8105_hw5_cw3747_files/figure-gfm/unnamed-chunk-9-1.png" width="90%" />

The sample average of $\hat{\mu}$ among tests that reject $H_0$ is not
approximately equal to the true $\mu$. We can see from the plot, when
the $\mu$ is around 1, conditioning on significance selects extreme
estimates, so the $\hat{\mu}$ shifted away from 0. It’s like a form of
selection bias. Only when the power is very high so that almost all
samples reject $H_0$, $\mu$ and $\hat{\mu}$ are close.

## Problem3

### data import

``` r
homicide = read_csv("./homicide-data.csv") |>
  janitor::clean_names() |>
  mutate(reported_date = ymd(reported_date))
```

    ## Rows: 52179 Columns: 12
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

    ## Warning: There was 1 warning in `mutate()`.
    ## ℹ In argument: `reported_date = ymd(reported_date)`.
    ## Caused by warning:
    ## !  2 failed to parse.

### describe

``` r
glimpse(homicide)
```

    ## Rows: 52,179
    ## Columns: 12
    ## $ uid           <chr> "Alb-000001", "Alb-000002", "Alb-000003", "Alb-000004", …
    ## $ reported_date <date> 2010-05-04, 2010-02-16, 2010-06-01, 2010-01-01, 2010-01…
    ## $ victim_last   <chr> "GARCIA", "MONTOYA", "SATTERFIELD", "MENDIOLA", "MULA", …
    ## $ victim_first  <chr> "JUAN", "CAMERON", "VIVIANA", "CARLOS", "VIVIAN", "GERAL…
    ## $ victim_race   <chr> "Hispanic", "Hispanic", "White", "Hispanic", "White", "W…
    ## $ victim_age    <chr> "78", "17", "15", "32", "72", "91", "52", "52", "56", "4…
    ## $ victim_sex    <chr> "Male", "Male", "Female", "Male", "Female", "Female", "M…
    ## $ city          <chr> "Albuquerque", "Albuquerque", "Albuquerque", "Albuquerqu…
    ## $ state         <chr> "NM", "NM", "NM", "NM", "NM", "NM", "NM", "NM", "NM", "N…
    ## $ lat           <dbl> 35.09579, 35.05681, 35.08609, 35.07849, 35.13036, 35.151…
    ## $ lon           <dbl> -106.5386, -106.7153, -106.6956, -106.5561, -106.5810, -…
    ## $ disposition   <chr> "Closed without arrest", "Closed by arrest", "Closed wit…

``` r
count(homicide, disposition, sort = TRUE)
```

    ## # A tibble: 3 × 2
    ##   disposition               n
    ##   <chr>                 <int>
    ## 1 Closed by arrest      25674
    ## 2 Open/No arrest        23583
    ## 3 Closed without arrest  2922

``` r
count(homicide, city, state, sort = TRUE) |> head(10)
```

    ## # A tibble: 10 × 3
    ##    city         state     n
    ##    <chr>        <chr> <int>
    ##  1 Chicago      IL     5535
    ##  2 Philadelphia PA     3037
    ##  3 Houston      TX     2942
    ##  4 Baltimore    MD     2827
    ##  5 Detroit      MI     2519
    ##  6 Los Angeles  CA     2257
    ##  7 St. Louis    MO     1677
    ##  8 Dallas       TX     1567
    ##  9 Memphis      TN     1514
    ## 10 New Orleans  LA     1434

``` r
n_rows   <- nrow(homicide)
n_cols   <- ncol(homicide)
n_cities <- dplyr::n_distinct(homicide$city)
n_states <- dplyr::n_distinct(homicide$state)

date_min <- suppressWarnings(min(homicide$reported_date, na.rm = TRUE))
date_max <- suppressWarnings(max(homicide$reported_date, na.rm = TRUE))

unsolved_flag <- homicide$disposition %in% c("Closed without arrest","Open/No arrest")
n_unsolved    <- sum(unsolved_flag, na.rm = TRUE)

top_cities <- homicide |>
  count(city, state, sort = TRUE) |>
  mutate(city_state = paste0(city, ", ", state)) |>
  slice_head(n = 5)
```

The raw homicide dataset contains 52179 observations and 12 variables
collected from 50 cities across 28 states. The data span from 2007-01-01
to 2017-12-31. And treating “Closed without arrest” and “Open/No arrest”
as unsolved, there are 26505 unsolved cases of all records. Cities with
the most cases include Chicago, IL (5535), Philadelphia, PA (3037), and
Houston, TX (2942).

### city state

``` r
library(stringr)

city_summary <- homicide |>
  mutate(
    city_state = str_c(city, ", ", state),
    unsolved = disposition %in% c("Closed without arrest", "Open/No arrest")
  ) |>
  group_by(city_state) |>
  summarise(
    total_homicides    = n(),
    unsolved_homicides = sum(unsolved, na.rm = TRUE),
    solved_homicides   = total_homicides - unsolved_homicides,
    pct_unsolved       = unsolved_homicides / total_homicides
  ) |>
  arrange(desc(total_homicides))
```

``` r
overall <- homicide |>
  mutate(unsolved = disposition %in% c("Closed without arrest","Open/No arrest")) |>
  summarise(
    total = n(),
    unsolved = sum(unsolved, na.rm = TRUE),
    pct = unsolved / total
  )

top_tot <- city_summary |> slice_max(total_homicides, n = 1)
top_uns <- city_summary |> slice_max(unsolved_homicides, n = 1)
```

There are 51 cities in the dataset. Overall, we observe 52179 homicides,
of which 26505 (50.8%) are unsolved. The city with the most homicides is
Chicago, IL (5535 cases), and it also has 4073 unsolved cases (73.6%).

### prop test fot bal

``` r
library(broom)

balt_row <- city_summary %>% 
  filter(city_state == "Baltimore, MD") %>%
  select(x = unsolved_homicides, n = total_homicides)


balt_row
```

    ## # A tibble: 1 × 2
    ##       x     n
    ##   <int> <int>
    ## 1  1825  2827

``` r
pt_balt <- prop.test(pull(balt_row, x), pull(balt_row, n))  #save the output of prop.test as an R object

balt_res <- tidy(pt_balt) |>
  select(p_hat = estimate, conf_low = conf.low, conf_high = conf.high)

balt_res
```

    ## # A tibble: 1 × 3
    ##   p_hat conf_low conf_high
    ##   <dbl>    <dbl>     <dbl>
    ## 1 0.646    0.628     0.663

``` r
p_hat <- balt_res$p_hat; ci_lo <- balt_res$conf_low; ci_hi <- balt_res$conf_high
```

In Baltimore, MD, the estimated proportion of unsolved homicides is
64.6%, 95% CI 62.8%–66.3%.

### prop test for all

``` r
library(purrr)

city_prop_all <- city_summary |>
  transmute(
    city_state,
    x = unsolved_homicides,
    n = total_homicides
  ) |>
  mutate(
    pt_obj = map2(x, n, ~ prop.test(.x, .y))
  ) |>
  mutate(
    pt_tidy = map(pt_obj, tidy)
  ) |>
  select(city_state, pt_tidy) |>
  tidyr::unnest(pt_tidy) |>
   transmute(
    city_state,
    p_hat    = estimate,
    conf_low = conf.low,
    conf_high= conf.high
  )
```

    ## Warning: There was 1 warning in `mutate()`.
    ## ℹ In argument: `pt_obj = map2(x, n, ~prop.test(.x, .y))`.
    ## Caused by warning in `prop.test()`:
    ## ! Chi-squared approximation may be incorrect

``` r
library(knitr)

knitr::kable(
  city_prop_all |> dplyr::arrange(desc(p_hat)),
  caption = "Estimated proportion of unsolved homicides and 95% CI by city"
)
```

| city_state         |     p_hat |  conf_low | conf_high |
|:-------------------|----------:|----------:|----------:|
| Chicago, IL        | 0.7358627 | 0.7239959 | 0.7473998 |
| New Orleans, LA    | 0.6485356 | 0.6231048 | 0.6731615 |
| Baltimore, MD      | 0.6455607 | 0.6275625 | 0.6631599 |
| San Bernardino, CA | 0.6181818 | 0.5576628 | 0.6753422 |
| Buffalo, NY        | 0.6122841 | 0.5687990 | 0.6540879 |
| Miami, FL          | 0.6048387 | 0.5685783 | 0.6400015 |
| Stockton, CA       | 0.5990991 | 0.5517145 | 0.6447418 |
| Detroit, MI        | 0.5883287 | 0.5687903 | 0.6075953 |
| Phoenix, AZ        | 0.5514223 | 0.5184825 | 0.5839244 |
| Denver, CO         | 0.5416667 | 0.4846098 | 0.5976807 |
| St. Louis, MO      | 0.5396541 | 0.5154369 | 0.5636879 |
| Oakland, CA        | 0.5364308 | 0.5040588 | 0.5685037 |
| Pittsburgh, PA     | 0.5340729 | 0.4942706 | 0.5734545 |
| Columbus, OH       | 0.5304428 | 0.5002167 | 0.5604506 |
| Jacksonville, FL   | 0.5111301 | 0.4820460 | 0.5401402 |
| Minneapolis, MN    | 0.5109290 | 0.4585150 | 0.5631099 |
| Houston, TX        | 0.5074779 | 0.4892447 | 0.5256914 |
| San Francisco, CA  | 0.5067873 | 0.4680516 | 0.5454433 |
| Boston, MA         | 0.5048860 | 0.4646219 | 0.5450881 |
| Los Angeles, CA    | 0.4900310 | 0.4692208 | 0.5108754 |
| Oklahoma City, OK  | 0.4851190 | 0.4467861 | 0.5236245 |
| Dallas, TX         | 0.4811742 | 0.4561942 | 0.5062475 |
| Savannah, GA       | 0.4674797 | 0.4041252 | 0.5318665 |
| Fort Worth, TX     | 0.4644809 | 0.4222542 | 0.5072119 |
| Baton Rouge, LA    | 0.4622642 | 0.4141987 | 0.5110240 |
| Tampa, FL          | 0.4567308 | 0.3881009 | 0.5269851 |
| Louisville, KY     | 0.4531250 | 0.4120609 | 0.4948235 |
| Indianapolis, IN   | 0.4493192 | 0.4223156 | 0.4766207 |
| Philadelphia, PA   | 0.4478103 | 0.4300380 | 0.4657157 |
| Cincinnati, OH     | 0.4452450 | 0.4079606 | 0.4831439 |
| Washington, DC     | 0.4379182 | 0.4112495 | 0.4649455 |
| Birmingham, AL     | 0.4337500 | 0.3991889 | 0.4689557 |
| San Antonio, TX    | 0.4285714 | 0.3947772 | 0.4630331 |
| Las Vegas, NV      | 0.4141926 | 0.3881284 | 0.4407395 |
| Omaha, NE          | 0.4132029 | 0.3653146 | 0.4627477 |
| Long Beach, CA     | 0.4126984 | 0.3629026 | 0.4642973 |
| Kansas City, MO    | 0.4084034 | 0.3803996 | 0.4370054 |
| New York, NY       | 0.3875598 | 0.3494421 | 0.4270755 |
| Albuquerque, NM    | 0.3862434 | 0.3372604 | 0.4375766 |
| Atlanta, GA        | 0.3833505 | 0.3528119 | 0.4148219 |
| San Diego, CA      | 0.3796095 | 0.3354259 | 0.4258315 |
| Sacramento, CA     | 0.3696809 | 0.3211559 | 0.4209131 |
| Durham, NC         | 0.3659420 | 0.3095874 | 0.4260936 |
| Nashville, TN      | 0.3624511 | 0.3285592 | 0.3977401 |
| Milwaukee, wI      | 0.3614350 | 0.3333172 | 0.3905194 |
| Fresno, CA         | 0.3470226 | 0.3051013 | 0.3913963 |
| Tulsa, OK          | 0.3310463 | 0.2932349 | 0.3711192 |
| Memphis, TN        | 0.3190225 | 0.2957047 | 0.3432691 |
| Charlotte, NC      | 0.2998544 | 0.2660820 | 0.3358999 |
| Richmond, VA       | 0.2634033 | 0.2228571 | 0.3082658 |
| Tulsa, AL          | 0.0000000 | 0.0000000 | 0.9453792 |

Estimated proportion of unsolved homicides and 95% CI by city
